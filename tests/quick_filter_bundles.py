#!/usr/bin/env python
"""
ğŸ” è¿‡æ»¤ Bundles - ä¿ç•™ [0.9x, 1.1x] èŒƒå›´å†…çš„ bundles

åœ¨ VS Code é‡Œï¼š
1. æ‰“å¼€è¿™ä¸ªæ–‡ä»¶
2. ç‚¹å‡»å³ä¸Šè§’çš„è¿è¡ŒæŒ‰é’® â–¶ï¸
3. æŸ¥çœ‹æŠ¥å‘Š

åŠŸèƒ½ï¼š
- è¿‡æ»¤æ‰ < 72 æˆ– > 88 addresses çš„ bundles
- ç”Ÿæˆæ–°çš„ parquet æ–‡ä»¶å’Œåœ°å›¾
- æŠ¥å‘Š dropped segments æ¯”ä¾‹å’Œåœ°å€åˆ†å¸ƒä»£è¡¨æ€§
"""

import sys
from pathlib import Path

# æ·»åŠ  src åˆ°è·¯å¾„
SRC = Path(__file__).parent.parent / "src"
if str(SRC) not in sys.path:
    sys.path.insert(0, str(SRC))

from sd311_fieldprep.utils import paths, load_sources
import geopandas as gpd
import numpy as np
import pandas as pd


def analyze_spatial_representativeness(original_gdf, filtered_gdf):
    """
    åˆ†æè¿‡æ»¤åçš„ç©ºé—´ä»£è¡¨æ€§ã€‚

    æ£€æŸ¥ï¼š
    1. åœ°ç†åˆ†å¸ƒï¼šdropped segments æ˜¯å¦é›†ä¸­åœ¨æŸäº›åŒºåŸŸ
    2. åœ°å€å¯†åº¦ï¼šdropped segments çš„åœ°å€å¯†åº¦æ˜¯å¦ä¸ä¿ç•™çš„ä¸åŒ
    """
    original_total = len(original_gdf)
    filtered_total = len(filtered_gdf)
    dropped_total = original_total - filtered_total

    # è®¡ç®— dropped segments
    dropped_gdf = original_gdf[~original_gdf.index.isin(filtered_gdf.index)]

    print("\n" + "="*70)
    print("ğŸ“Š ç©ºé—´ä»£è¡¨æ€§åˆ†æ")
    print("="*70)

    # 1. åŸºæœ¬ç»Ÿè®¡
    print(f"\n1ï¸âƒ£ åŸºæœ¬ç»Ÿè®¡:")
    print(f"   åŸå§‹ segments: {original_total:,}")
    print(f"   ä¿ç•™ segments: {filtered_total:,}")
    print(f"   ä¸¢å¼ƒ segments: {dropped_total:,}")
    print(f"   ä¸¢å¼ƒæ¯”ä¾‹: {dropped_total/original_total*100:.2f}%")

    # 2. åœ°å€ç»Ÿè®¡
    original_addrs = original_gdf['sfh_addr_count'].sum()
    filtered_addrs = filtered_gdf['sfh_addr_count'].sum()
    dropped_addrs = original_addrs - filtered_addrs

    print(f"\n2ï¸âƒ£ åœ°å€ç»Ÿè®¡:")
    print(f"   åŸå§‹åœ°å€: {int(original_addrs):,}")
    print(f"   ä¿ç•™åœ°å€: {int(filtered_addrs):,}")
    print(f"   ä¸¢å¼ƒåœ°å€: {int(dropped_addrs):,}")
    print(f"   ä¸¢å¼ƒæ¯”ä¾‹: {dropped_addrs/original_addrs*100:.2f}%")

    # 3. åœ°å€å¯†åº¦åˆ†å¸ƒ
    print(f"\n3ï¸âƒ£ åœ°å€å¯†åº¦åˆ†æ:")

    original_density = original_gdf['sfh_addr_count'].describe()
    filtered_density = filtered_gdf['sfh_addr_count'].describe()
    dropped_density = dropped_gdf['sfh_addr_count'].describe() if len(dropped_gdf) > 0 else None

    print(f"\n   åŸå§‹ segments åœ°å€å¯†åº¦:")
    print(f"      Mean: {original_density['mean']:.1f}")
    print(f"      Median: {original_density['50%']:.1f}")
    print(f"      Std: {original_density['std']:.1f}")

    print(f"\n   ä¿ç•™ segments åœ°å€å¯†åº¦:")
    print(f"      Mean: {filtered_density['mean']:.1f}")
    print(f"      Median: {filtered_density['50%']:.1f}")
    print(f"      Std: {filtered_density['std']:.1f}")

    if dropped_density is not None:
        print(f"\n   ä¸¢å¼ƒ segments åœ°å€å¯†åº¦:")
        print(f"      Mean: {dropped_density['mean']:.1f}")
        print(f"      Median: {dropped_density['50%']:.1f}")
        print(f"      Std: {dropped_density['std']:.1f}")

    # 4. ç©ºé—´åˆ†å¸ƒæ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼šåŸºäº centroid çš„ç©ºé—´èŒƒå›´ï¼‰
    print(f"\n4ï¸âƒ£ ç©ºé—´åˆ†å¸ƒæ£€æŸ¥:")

    original_cents = original_gdf.geometry.centroid
    filtered_cents = filtered_gdf.geometry.centroid

    original_bounds = original_gdf.total_bounds
    filtered_bounds = filtered_gdf.total_bounds

    bounds_change = [
        abs(filtered_bounds[i] - original_bounds[i]) / (original_bounds[2] - original_bounds[0])
        for i in range(4)
    ]

    print(f"   åŸå§‹è¾¹ç•Œ: ({original_bounds[0]:.0f}, {original_bounds[1]:.0f}) - ({original_bounds[2]:.0f}, {original_bounds[3]:.0f})")
    print(f"   ä¿ç•™è¾¹ç•Œ: ({filtered_bounds[0]:.0f}, {filtered_bounds[1]:.0f}) - ({filtered_bounds[2]:.0f}, {filtered_bounds[3]:.0f})")
    print(f"   è¾¹ç•Œå˜åŒ–: {max(bounds_change)*100:.2f}%")

    # 5. Bundle å¤§å°åˆ†å¸ƒ
    print(f"\n5ï¸âƒ£ Bundle å¤§å°åˆ†å¸ƒ:")

    original_bundle_sizes = original_gdf.groupby('bundle_id')['sfh_addr_count'].sum()
    filtered_bundle_sizes = filtered_gdf.groupby('bundle_id')['sfh_addr_count'].sum()

    print(f"   åŸå§‹ bundles: {len(original_bundle_sizes):,}")
    print(f"      Min: {int(original_bundle_sizes.min())}, Max: {int(original_bundle_sizes.max())}")
    print(f"      Mean: {original_bundle_sizes.mean():.1f}, Median: {original_bundle_sizes.median():.1f}")

    print(f"\n   è¿‡æ»¤å bundles: {len(filtered_bundle_sizes):,}")
    print(f"      Min: {int(filtered_bundle_sizes.min())}, Max: {int(filtered_bundle_sizes.max())}")
    print(f"      Mean: {filtered_bundle_sizes.mean():.1f}, Median: {filtered_bundle_sizes.median():.1f}")

    # 6. ç»“è®º
    print(f"\n6ï¸âƒ£ ä»£è¡¨æ€§ç»“è®º:")

    addr_loss_pct = dropped_addrs/original_addrs*100
    seg_loss_pct = dropped_total/original_total*100
    bounds_change_pct = max(bounds_change)*100

    issues = []

    if addr_loss_pct > 10:
        issues.append(f"âš ï¸  åœ°å€ä¸¢å¤±è¾ƒå¤š ({addr_loss_pct:.1f}%)")
    else:
        issues.append(f"âœ… åœ°å€ä¸¢å¤±å¯æ¥å— ({addr_loss_pct:.1f}%)")

    if seg_loss_pct > 15:
        issues.append(f"âš ï¸  è·¯æ®µä¸¢å¤±è¾ƒå¤š ({seg_loss_pct:.1f}%)")
    else:
        issues.append(f"âœ… è·¯æ®µä¸¢å¤±å¯æ¥å— ({seg_loss_pct:.1f}%)")

    if bounds_change_pct > 5:
        issues.append(f"âš ï¸  ç©ºé—´èŒƒå›´æ˜æ˜¾å˜åŒ– ({bounds_change_pct:.1f}%)")
    else:
        issues.append(f"âœ… ç©ºé—´èŒƒå›´åŸºæœ¬ä¸å˜ ({bounds_change_pct:.1f}%)")

    # å¯†åº¦ä»£è¡¨æ€§
    if dropped_density is not None:
        density_diff = abs(dropped_density['mean'] - filtered_density['mean']) / original_density['mean']
        if density_diff > 0.2:
            issues.append(f"âš ï¸  ä¸¢å¼ƒçš„ segments åœ°å€å¯†åº¦æ˜æ˜¾ä¸åŒ ({density_diff*100:.1f}%)")
        else:
            issues.append(f"âœ… åœ°å€å¯†åº¦åˆ†å¸ƒä»£è¡¨æ€§è‰¯å¥½")

    for issue in issues:
        print(f"   {issue}")

    print()


def filter_bundles():
    print("\n" + "="*70)
    print("ğŸ” è¿‡æ»¤ Bundles - ä¿ç•™ [0.9x, 1.1x] èŒƒå›´")
    print("="*70)

    # ========== é…ç½®åŒºåŸŸ ==========
    SESSION = "DH"
    TARGET_ADDRS = 80
    LOWER_BOUND = TARGET_ADDRS * 0.9  # 72
    UPPER_BOUND = TARGET_ADDRS * 1.1  # 88
    # =============================

    print(f"\nğŸ“‹ é…ç½®:")
    print(f"   Target: {TARGET_ADDRS} addresses")
    print(f"   ä¸‹é™: {int(LOWER_BOUND)} addresses (0.9x)")
    print(f"   ä¸Šé™: {int(UPPER_BOUND)} addresses (1.1x)")

    try:
        # åŠ è½½æ•°æ®
        print("\n[1/4] åŠ è½½æ•°æ®...")
        streets, addrs, ds, pr = load_sources()
        seg_id = pr["fields"]["streets_segment_id"]

        root, cfg, out_root = paths()
        bundle_dir = out_root / "bundles" / SESSION
        bundle_file = bundle_dir / "bundles_multibfs_regroup.parquet"

        if not bundle_file.exists():
            print(f"\nâŒ é”™è¯¯: æœªæ‰¾åˆ°æ–‡ä»¶ {bundle_file}")
            print(f"   è¯·å…ˆè¿è¡Œ quick_comparison_hard_constraint.py")
            return

        bundles = gpd.read_parquet(bundle_file)
        print(f"   âœ“ åŠ è½½äº† {len(bundles)} ä¸ªè·¯æ®µ")
        print(f"   âœ“ åŒ…å« {bundles['bundle_id'].nunique()} ä¸ª bundles")

        # è¿‡æ»¤ bundles
        print(f"\n[2/4] è¿‡æ»¤ bundles...")

        # è®¡ç®—æ¯ä¸ª bundle çš„æ€»åœ°å€æ•°
        bundle_sizes = bundles.groupby('bundle_id')['sfh_addr_count'].sum()

        # æ‰¾å‡ºç¬¦åˆæ¡ä»¶çš„ bundles
        valid_bundles = bundle_sizes[
            (bundle_sizes >= LOWER_BOUND) &
            (bundle_sizes <= UPPER_BOUND)
        ].index

        # æ‰¾å‡ºä¸ç¬¦åˆæ¡ä»¶çš„ bundles
        too_small = bundle_sizes[bundle_sizes < LOWER_BOUND]
        too_large = bundle_sizes[bundle_sizes > UPPER_BOUND]

        print(f"   åŸå§‹ bundles: {len(bundle_sizes)}")
        print(f"   âœ… ç¬¦åˆæ¡ä»¶: {len(valid_bundles)} bundles")
        print(f"   âŒ å¤ªå° (< {int(LOWER_BOUND)}): {len(too_small)} bundles")
        print(f"   âŒ å¤ªå¤§ (> {int(UPPER_BOUND)}): {len(too_large)} bundles")

        if len(too_small) > 0:
            print(f"      æœ€å° bundle: {int(too_small.min())} addresses")
        if len(too_large) > 0:
            print(f"      æœ€å¤§ bundle: {int(too_large.max())} addresses")

        # è¿‡æ»¤ GeoDataFrame
        filtered_bundles = bundles[bundles['bundle_id'].isin(valid_bundles)].copy()

        print(f"\n   è¿‡æ»¤ç»“æœ:")
        print(f"   ä¿ç•™è·¯æ®µ: {len(filtered_bundles)} / {len(bundles)}")
        print(f"   ä¸¢å¼ƒè·¯æ®µ: {len(bundles) - len(filtered_bundles)}")
        print(f"   ä¸¢å¼ƒæ¯”ä¾‹: {(len(bundles) - len(filtered_bundles))/len(bundles)*100:.2f}%")

        # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
        print(f"\n[3/4] ä¿å­˜è¿‡æ»¤åçš„æ•°æ®...")

        filtered_file = bundle_dir / "bundles_multibfs_regroup_filtered.parquet"
        filtered_bundles.to_parquet(filtered_file, index=False)
        print(f"   âœ“ å·²ä¿å­˜: {filtered_file}")

        # ç”Ÿæˆåœ°å›¾
        print(f"\n[4/5] ç”Ÿæˆåœ°å›¾...")

        try:
            import folium
            from sd311_fieldprep.utils import folium_map

            # ä¿ç•™çš„ bundles åœ°å›¾
            map_file = bundle_dir / "bundles_multibfs_regroup_filtered_map.html"

            viz = filtered_bundles.dropna(subset=["bundle_id"]).to_crs(4326)
            tooltip_cols = [seg_id, "bundle_id", "bundle_seg_count", "bundle_addr_total"]

            m = folium_map(viz, color_col="bundle_id", tooltip_cols=tooltip_cols)
            m.save(str(map_file))
            print(f"   âœ“ ä¿ç•™çš„åœ°å›¾å·²ä¿å­˜: {map_file}")
        except Exception as e:
            print(f"   âš ï¸  ä¿ç•™çš„åœ°å›¾ç”Ÿæˆå¤±è´¥: {e}")

        # ç­›æ‰çš„ bundles åœ°å›¾
        print(f"\n[5/5] ç”Ÿæˆç­›æ‰çš„ bundles åœ°å›¾...")

        try:
            # æ‰¾å‡ºè¢«ç­›æ‰çš„ bundles
            dropped_bundle_ids = bundle_sizes[
                (bundle_sizes < LOWER_BOUND) | (bundle_sizes > UPPER_BOUND)
            ].index
            dropped_bundles = bundles[bundles['bundle_id'].isin(dropped_bundle_ids)].copy()

            if not dropped_bundles.empty:
                import folium
                from sd311_fieldprep.utils import folium_map

                dropped_map_file = bundle_dir / "bundles_multibfs_regroup_dropped_map.html"

                viz_dropped = dropped_bundles.dropna(subset=["bundle_id"]).to_crs(4326)
                tooltip_cols = [seg_id, "bundle_id", "bundle_seg_count", "bundle_addr_total"]

                m_dropped = folium_map(viz_dropped, color_col="bundle_id", tooltip_cols=tooltip_cols)
                m_dropped.save(str(dropped_map_file))
                print(f"   âœ“ ç­›æ‰çš„åœ°å›¾å·²ä¿å­˜: {dropped_map_file}")
                print(f"   ğŸ“Š ç­›æ‰çš„ bundles: {len(dropped_bundle_ids)} bundles, {len(dropped_bundles)} segments")
            else:
                print(f"   â„¹ï¸  æ²¡æœ‰è¢«ç­›æ‰çš„ bundles")
                dropped_map_file = None
        except Exception as e:
            print(f"   âš ï¸  ç­›æ‰çš„åœ°å›¾ç”Ÿæˆå¤±è´¥: {e}")
            dropped_map_file = None

        # åˆ†æç©ºé—´ä»£è¡¨æ€§
        analyze_spatial_representativeness(bundles, filtered_bundles)

        # æœ€ç»ˆæŠ¥å‘Š
        print("="*70)
        print("âœ… è¿‡æ»¤å®Œæˆ!")
        print("="*70)
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  - è¿‡æ»¤åæ•°æ®: {filtered_file}")
        print(f"  - ä¿ç•™çš„åœ°å›¾: {map_file}")
        if dropped_map_file:
            print(f"  - ç­›æ‰çš„åœ°å›¾: {dropped_map_file}")
        print()

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    filter_bundles()
